# RAG for chatbot - Llama Neurology assistant 🦙🧠💬

<div align="center">
    <img src="./static/images/top.PNG" alt="RAG Architecture" />
</div>

# Introduction
---

I’ve been working on a small project: a Retrieval Augmented Generation (RAG)-based “Neurological Assistant” 🧠 that uses Meta’s [LLaMA 3.2 (3B)](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/) model as its text generator. To power it up with real knowledge, I’m using the “all-MiniLM-L6-v2” sentence transformer to create embeddings from some books in PDF form on the human brain, cognition, and behavior. All these embeddings get stashed in a [Pinecone](https://www.pinecone.io/) vector database 🌲, so the assistant can fetch the info it needs whenever it’s stuck on a tough question. On top of that, I’ve also rolled out a user interface 💻 to make the whole thing more user-friendly. Now, instead of just a command-line tool, you’ve got a neat, interactive way to chat and learn.

The idea is to:  

✅ Turn complex neuroscience books into friendly conversations, making exploration less intimidating and more fun.  
✅ Foster interactive, engaging learning that helps you absorb and recall information faster.  
✅ Quickly retrieve learned knowledge to deepen understanding and connect new ideas.  
✅ Transform passive reading into a two-way dialogue, encouraging curiosity and insight.  
✅ Ensure complete privacy: Locally deployed, so your questions and data stay with you and are never stored by others.🛡️  
✅ Customize learning: Tailor the assistant to your specific interests, topics, or data for a more personalized experience.🎯  

Want to build your own chatbot to assist with reading and learning? 📚
Whether it’s tackling complex topics 🧠, organizing daily tasks ✅, or exploring areas of interest 🔍, a RAG-powered assistant can make knowledge more accessible and practical.

For full installation instructions, see INSTALL.md. 🚀



# RAG
---

<div align="center">
    <img src="./static/images/RAG.PNG" alt="RAG Architecture" title="RAG Architecture Diagram" />
    <p style="font-size: small;">Source: <a href="https://valueminer.eu/de/retrieval-augmented-generation-rag" target="_blank">ValueMiner - Retrieval Augmented Generation (RAG)</a></p>
</div>

Retrieval-Augmented Generation (RAG) is an AI framework that combines retrieval-based methods with generative models. It retrieves relevant external knowledge (e.g., documents) to improve the quality, accuracy, and reliability of generated responses.

Use Cases:  
    ✅ Question Answering: Providing accurate answers based on retrieved documents.  
    ✅ Customer Support: Query resolution using product manuals or FAQs.  
    ✅ Educational Tools: Summarizing textbooks or academic resources.  
    ✅ Knowledge Assistants: Assisting professionals with real-time information retrieval.  

RAG enhances generative AI by grounding outputs in up-to-date, factual information.



# Challenges and Optimizations
---

This is a small project I implemented in a short time to learn about RAG (Retrieval-Augmented Generation) 🧠, so there’s still room for improvement. If you plan to use this, I recommend modifying or enhancing some methods ✨ to improve the quality of the “Answer” generated by LLaMA 3.2 on the UI 🖥️.

For instance, I attempted to implement short-term conversation memory (commented in the app.py script) 💬 to make the conversation smoother and more contextually connected. However, I still encountered challenges due to the Prompt Template I used.
For example: 
    User: "what is acne?"
    Bot: "Acne is ..."
    User: "How to prevent that?"
    Bot: "(make no sense answer - Although the memory are there)"
🛠️ Incorporating better text extraction techniques 📄 or advanced analysis methods could further refine the answers.

Another promising approach is thought chaining 🔗 to produce more accurate responses. However, this increases inference time ⏳ significantly. On my setup with 16GB RAM and an NVIDIA GTX 1080 Ti (6GB) 🖥️, generating a single complete answer takes 3-4 minutes ⏱️. This makes it difficult to implement chaining or introduce an answer evaluator (judger) 🧐.

When testing on a system with an NVIDIA RTX A4000 ⚡, inference time dropped to 2-3 seconds per answer 🚀. Depending on your hardware, you might want to consider using a smaller or larger model to balance performance ⚖️ and accuracy 🎯.

## Phần này mai a xem lại đã nghen 
    ## Prompt extractor:
    [Llama3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) is really really good at giving out answer, when integrating in RAG-based system. But the output prompt from Llama always include the Prompt_Template that i give in itself and the retrieval context, and also some extra automatically generated questions and answers. So the model doesnt only stop with giving the answer from the user question

    My extractor located in the 
    ...Definition what's RAG, structure + usecases



# Installation
---

For full installation instructions, see [INSTALL.md](INSTALL.md).



# Some conversations
---

Thêm ảnh dô

<div align="center">
    <img src="./static/images/4.PNG" alt="RAG Architecture" />
</div>
